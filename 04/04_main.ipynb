{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Under the Hood: Training a Digit Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline binary digit classifier 3 vs. 7\n",
    "*Practicing PyTorch and build a simple classifier based on similarity with two typical digits.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('valid'),Path('labels.csv'),Path('train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digit(set, d):\n",
    "    files = (path/set/d).ls()\n",
    "    display(Image.open(files[0]))\n",
    "    tensors = [tensor(Image.open(f)) for f in files]\n",
    "    return torch.stack(tensors).float() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA40lEQVR4nGNgGEiQ8ffv38pKH+yScnP+/Pn75/uHDx+mqEOFGBGyzOIMrf8ZGLyFGW7o4DJePvnSewsIkwlDUixT6y4OfYbL3n081o9VSiTxy58/Z7WwSTFJn/nz5+k+CaxyUn/+fnunit063iN//k7E4RQGBr+j//+2BuOS9Z30/c+PtlBc0tPnvf1zSw6n2RMP/mnFLdv55zksaDGCL8KZ4fsbFBF+GShDcMGHP4ecUeQs9+dLS0tLSyctPP7ncwVaCFl++AMBf//s80ASh0T2nAQGBgYGhp2H/0/9gtOl9AEAHtFYA7M5jDIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x12B3529D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvklEQVR4nN2Quw4BURRF151MoTEZiYKSUkThC1QSj47WD+k0ep1COQqhVXpUCiJaoqBCY47GZK4Z1wfY1dlZ2ecF/6pKb7D3u4Gzodl11WKDkqar0hZKRIPM2k6thhI4nb1lP+yjAChmC6k5MAV8qU9MUzvil4LaisJWODIOE2x3Rgje3QQzVY7mtqLVUdiAhxGWYRRf4621r30gmhTtzC+njI0wn+MaOvsTJh3dRZL3m2DW6jk0L3Tg8iOq6wUDajDxk8YtwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x12B644040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors3 = load_digit('train', '3')\n",
    "tensors7 = load_digit('train', '7')\n",
    "tensors3.shape, tensors7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = tensors3.mean(0)\n",
    "mean7 = tensors7.mean(0)\n",
    "show_image(mean3)\n",
    "show_image(mean7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    # a, b can have different ranks, broadcasting\n",
    "    return (a - b).abs().mean((-1, -2))\n",
    "\n",
    "def is3(x):\n",
    "    return dist(x, mean3) < dist(x, mean7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(False))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is3(tensors3[0]), is3(tensors7[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABIUlEQVR4nGNgoDEQzp+97t+/f3tSsUku/gsBP10ZMSVD/8KACEyIBS756z/j1+VvGJTCGMKmYWr19ldkYGBw+/t3Ck53uf39ewvGZkKTU0/BpUui8tS3v3//1mLKKEw79PTv379///29IIEu57sD7pW/W8TQJFdDxH/fPfbr799cVDmh13///v3790Y2A4Pcrb+rUSW1jv79/a7Hmp+BgYGh7e9TSVRZ8+2+MOaiv3/N4OIyQVKIUGT3Pvz733MZBgYGSNhyzeM98G35netqfxUY9LxMGRj+n3qCMLL/77+/f7+//fbt77+/f//9/ffRHMk+5t0IP/799/eAFopr2HZChP/+/ftxfi4PTBga66yGJvoMvuLXj17ZfQM97AYRAADfvp8MM8h4ggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x12B6B7670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuUlEQVR4nGNgGBmAkYGBgYGhpOv5HgYGxv9bjR68Vzx35RmKkpK///7+heM+VP1yd5Elb3CjGMtgpPNeyZDx3TYGBobgVEbJl7hc0fvvEicuOakXf5fD2EzokomiDGtx+m3N39s45YTf/luIU3Lm3x8OcA66nYIMiw7gklQKYXiP09Tp///pIXioOln1/n/7gUtj8t9/zUhcVJ3BDAzncGnkvYsnXPX//k1F5qMY+/sPblMZGM6jhisAW9xORjYP8OAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x12B6C9B50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tensors3 = load_digit('valid', '3')\n",
    "valid_tensors7 = load_digit('valid', '7')\n",
    "valid_tensors3.shape, valid_tensors7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9168), tensor(0.9854), tensor(0.9511))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc3 = is3(valid_tensors3).float().mean()\n",
    "acc7 = 1 - is3(valid_tensors7).float().mean()\n",
    "acc3, acc7, (acc3 + acc7) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to a ML problem\n",
    "- Each position in an image contributes to the classification decision. For instance, pixels at the bottom right corners mean it's less likely that the image is a 7. We can consider each position as a **weight**.\n",
    "- The intensity of each pixel also contributes.\n",
    "- We can compute some value based on these two elements that can be used to help the classification. And we want to find the weights so that the computed values can be used to classify as accurate as possible.\n",
    "\n",
    "### Process\n",
    "1. Initialize the weights.\n",
    "1. For each image, use these weights to predict whether it appears to be a 3 or a 7.\n",
    "1. Based on these predictions, calculate how good the model is (its loss).\n",
    "1. Calculate the gradient, which measures for each weight, how changing that weight would change the loss\n",
    "1. Step (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and repeat the process.\n",
    "1. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(tensors3, tensors7, bs=256, shuffle=False):\n",
    "    X = torch.cat([tensors3, tensors7]).view(-1, 28*28)\n",
    "    y = tensor([1.0] * len(tensors3) + [0.0] * len(tensors7)).unsqueeze(1)\n",
    "    print(X.shape, y.shape)\n",
    "    ds = TensorDataset(X, y)\n",
    "    dl = DataLoader(ds, bs=bs, shuffle=shuffle)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12396, 784]) torch.Size([12396, 1])\n",
      "torch.Size([2038, 784]) torch.Size([2038, 1])\n"
     ]
    }
   ],
   "source": [
    "dl = create_dl(tensors3, tensors7, bs=256, shuffle=True)\n",
    "valid_dl = create_dl(valid_tensors3, valid_tensors7, bs=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(xb):\n",
    "    return xb@weights + bias\n",
    "\n",
    "def validate_epoch(model):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for xb, yb in valid_dl:\n",
    "        all_preds.append(model(xb).sigmoid())\n",
    "        all_targets.append(yb)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    return ((all_preds > 0.5) == all_targets).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the process from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964\n",
      "0.971\n",
      "0.970\n",
      "0.976\n",
      "0.976\n",
      "0.976\n",
      "0.978\n",
      "0.981\n",
      "0.979\n",
      "0.981\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the weights\n",
    "weights = torch.randn((28 * 28, 1), requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "lr = 1\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    for xb, yb in dl:\n",
    "        # 2. Forward pass, compute predictions\n",
    "        preds = xb@weights + bias\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        preds = preds.sigmoid().clamp(1e-6, 1 - 1e-6)\n",
    "        loss = (-yb*torch.log(preds) - (1-yb)*torch.log(1-preds)).mean()\n",
    "        \n",
    "        # 4. Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        with torch.no_grad():\n",
    "            for p in [weights, bias]:\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "\n",
    "    # Validation     \n",
    "    print(f'{validate_epoch(linear):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the process with PyTorch classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify:\n",
    "1. Use `nn.Linear` which does both weights initialization (Step 1) and linear transformation (Step 2).\n",
    "1. Use `nn.BCEWithLoss` as a loss function (Step 3)\n",
    "1. Use an optimizer `torch.optim.SGD` which handles step and zero grad (Step 4 and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974\n",
      "0.979\n",
      "0.981\n",
      "0.981\n",
      "0.980\n",
      "0.982\n",
      "0.982\n",
      "0.981\n",
      "0.982\n",
      "0.981\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the weights\n",
    "linear_model = nn.Linear(28 * 28, 1)\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    for xb, yb in dl:\n",
    "        # 2. Forward pass, compute predictions\n",
    "        preds = linear_model(xb)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = nn.BCEWithLogitsLoss()(preds, yb)\n",
    "        \n",
    "        # 4. Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation     \n",
    "    print(f'{validate_epoch(linear_model):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up in a fastai learner class\n",
    "fastai provides a class to encapsulate the training process. Besides a standard model architecture, a loss function and an optimizer, it requires two extra pieces for validation:\n",
    "- a `DataLoaders` instance which simply combines train and validation standard data loaders \n",
    "- a list of metrics to compute for the validation set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds > 0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.078729</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.059303</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051856</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.046706</td>\n",
       "      <td>0.055486</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043323</td>\n",
       "      <td>0.054798</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040702</td>\n",
       "      <td>0.054416</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.039523</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037756</td>\n",
       "      <td>0.051930</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.036168</td>\n",
       "      <td>0.051740</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037250</td>\n",
       "      <td>0.059670</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28, 1), loss_func=nn.BCEWithLogitsLoss(), opt_func=SGD, metrics=batch_accuracy)\n",
    "learn.fit(10, lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basics above, it's easy to extend the linear model to a (deep) neural network. The only change required is the forward pass in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.688396</td>\n",
       "      <td>0.680210</td>\n",
       "      <td>0.814524</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.628947</td>\n",
       "      <td>0.426033</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293682</td>\n",
       "      <td>0.076844</td>\n",
       "      <td>0.971050</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.141844</td>\n",
       "      <td>0.063936</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083077</td>\n",
       "      <td>0.054872</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055967</td>\n",
       "      <td>0.050906</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.044502</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.038228</td>\n",
       "      <td>0.048978</td>\n",
       "      <td>0.984298</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031858</td>\n",
       "      <td>0.045506</td>\n",
       "      <td>0.985770</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.984789</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.987733</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.023842</td>\n",
       "      <td>0.039471</td>\n",
       "      <td>0.987733</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021642</td>\n",
       "      <td>0.033602</td>\n",
       "      <td>0.989205</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019428</td>\n",
       "      <td>0.034138</td>\n",
       "      <td>0.989205</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.989696</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.015347</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.990677</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.990186</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.028653</td>\n",
       "      <td>0.992149</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deep_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "learn = Learner(dls, deep_net, loss_func=nn.BCEWithLogitsLoss(), opt_func=SGD, metrics=batch_accuracy)\n",
    "learn.fit(20, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a DNN, choosing a large learning rate is unstable. With my 4-layer DNN, setting `lr=1` sometimes gives above 99% but sometimes 50%. Setting a small `lr=0.1` and train for a larger number of epochs gives a more consistent result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
