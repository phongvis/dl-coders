{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Digit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes from [Chapter 4](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb) of the Practical Deep Learning for Coders book.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter dives into implementing a binary classifier using `PyTorch`. It starts with a from-scratch implementation then simplifying it with useful classes from PyTorch and `fastai`. The task is to build a binary digit classifier to distinguish 3 and 7 using MNIST dataset. \n",
    "\n",
    "In this blog post, I will highlight the parts that excite me, whereas my full implementation can be found in this [notebook](https://github.com/phongvis/dl-coders/blob/master/04/04_full.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training process\n",
    "![machine-learning-training](04_blog_files/training_process.svg) \"Source: https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb\"\n",
    "\n",
    "From the figure in the book, the process consists of an initialization and an iterative loop:\n",
    ">S1. **Init.** Initialize the weights.\n",
    ">\n",
    ">REPEAT\n",
    ">\n",
    ">S2. **Predict.** For each image, use the current weights and the pixel values to compute a derived value that can be used to predict whether it appears to be a 3 or a 7.\n",
    ">\n",
    ">S3. **Loss.** Measure the goodness of the model based on these predictions and real labels. Loss is the opposite measurement to the goodness as we want to minimize the loss.\n",
    ">\n",
    ">S4. **Gradient.** Calculate the derivative of the loss with respect to the weights. This measures for each weight, how changing that weight would change the loss.\n",
    ">\n",
    ">S5. **Step**. Change all the weights with an amount proportional to the gradients.\n",
    ">\n",
    ">UNTIL stopping criteria meet (such as when the model doesn't improve or get worse or already train for long enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from scratch with PyTorch basics\n",
    "#### Weight initialization\n",
    "First, what are the **weights**? Naturally, each pixel in the image can contribute to the classification decision. For instance, pixels at the bottom right corner could indicate that it's less likely that the image is a 7. Thus, one approach is to *consider each position as a parameter or a weight*. So, we have `28 * 28 = 784` weights. We are building a linear model, so besides the weights, we also need a **bias** term.\n",
    "\n",
    "To initialize the weights and bias, we can take the simplest approach, assign the values randomly or drawing them from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn((28 * 28, 1), requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The iteration\n",
    "We usually train a model with multiple passes or epochs using gradient descent optimization algorithm. An **epoch** is a visit through the entire dataset. The *predict-loss-gradient-step* loop operates on a number of data points, called a **mini-batch**. If the size of the mini batch is 1, we have *stochastic* gradient descent, which could make the weights jumping a lot. If the size of mini batch is the same as the size of the entire dataset, we have *batch* gradient descent, which could be slow. In practice, the size is in between these two extreme cases and depends on the memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb, yb in dl: # dl is a DataLoader instance in PyTorch for batching\n",
    "        # Step 2-5 are here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "With a linear model, the prediction is a linear combination of weights and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xb@weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "A log loss can be used here which penalizes confidently wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.sigmoid().clamp(1e-6, 1 - 1e-6) # Avoid log(0)\n",
    "loss = (-yb*torch.log(preds) - (1-yb)*torch.log(1-preds)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "This is the only place where we actually need PyTorch for this from scratch implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling `backward()` the weights and bias will have the computed gradients stored in `.grad` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight update\n",
    "Updating the weights and bias with an amount proportional to the gradients (controlled by the learning rate `lr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # We don't want PyTorch calculates gradients for this weight update operation\n",
    "    for p in [weights, bias]:\n",
    "        p -= p.grad * lr\n",
    "        p.grad.zero_() # Tell PyTorch not accumulate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying with PyTorch classes\n",
    "PyTorch helps us simplify the code.\n",
    "1. Use `nn.Linear` which does both weights initialization (Step 1) and linear transformation (Step 2).\n",
    "1. Use `nn.BCEWithLoss` as a loss function (Step 3)\n",
    "1. Use an optimizer `torch.optim.SGD` which handles step and zero grad (Step 4 and 5).\n",
    "\n",
    "This side-by-side comparison could help see the changes.\n",
    "![code comparison](04_blog_files/comparison.png) \"Left: from scratch implementation. Right: simplification with PyTorch classes.\"\n",
    "\n",
    "For this simple linear model, the benefit might not be so great. But PyTorch provides ways to build more complex deep network architecture and a large number of loss functions and optimization algorithms. For example, here is a 4-layer deep neural net I built to achieve above 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up with fastai classes\n",
    "`fastai` makes the training process more convenient for us by encapsulating the training process in the `Learner` class. I will also output a nice table of useful information. Besides a standard model architecture, a loss function and an optimizer, it requires two extra pieces for validation:\n",
    "- a `DataLoaders` instance which simply combines train and validation standard data loaders \n",
    "- a list of metrics to compute for the validation set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)\n",
    "learn = Learner(dls, deep_net, loss_func=nn.BCEWithLogitsLoss(), opt_func=SGD, metrics=batch_accuracy)\n",
    "learn.fit(20, lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
